{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "### <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n",
    "\n",
    "#### Maximum Points: 100\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n",
    "        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Dev Scalars Log Link</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n",
    "    </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">0.1 Clone git repository to Kaggle workspace</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_inference_only = False\n",
    "\n",
    "#delete directory if exists\n",
    "import shutil\n",
    "import os\n",
    "if os.path.exists('prj-2-opencv-dl-pytorch'):\n",
    "    shutil.rmtree('prj-2-opencv-dl-pytorch')\n",
    "# Clone repository from GitHub\n",
    "!git clone https://github.com/ramabyg/prj-2-opencv-dl-pytorch.git\n",
    "\n",
    "# Add to Python path\n",
    "import sys\n",
    "sys.path.insert(0, '/kaggle/working/prj-2-opencv-dl-pytorch')\n",
    "\n",
    "print(\"[OK] Repository cloned and added to path\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">0.2 Import modules </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import get_config\n",
    "from src.datamodule import KenyanFood13DataModule\n",
    "from src.model import KenyanFood13Classifier\n",
    "from src.trainer import train_model\n",
    "from src.utils import calculate_dataset_mean_std\n",
    "\n",
    "print(\"[OK] All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n",
    "\n",
    "In this section, you have to write a class or methods, which will be used to get training and validation data loader.\n",
    "\n",
    "You need to write a custom dataset class to load data.\n",
    "\n",
    "**Note; There is   no separate validation data. , You will thus have to create your own validation set, by dividing the train data into train and validation data. Usually, we do 80:20 ratio for train and validation, respectively.**\n",
    "\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "class KenyanFood13Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "    ....\n",
    "    ...\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "def get_data(args1, *args):\n",
    "    ....\n",
    "    ....\n",
    "    return train_loader, test_loader\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Please refer to src/datamodule.py and src/dataset.py.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2. Configuration [5 Points]</font>\n",
    "\n",
    "**Define your configuration here.**\n",
    "\n",
    "For example:\n",
    "\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 10 \n",
    "    epochs_count: int = 50  \n",
    "    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n",
    "    log_interval: int = 5  \n",
    "    test_interval: int = 1  \n",
    "    data_root: str = \"/kaggle/input/opencv-pytorch-project-2-classification-round-3\" \n",
    "    num_workers: int = 2  \n",
    "    device: str = 'cuda'  \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from src.config import get_config\n",
    "# Get configurations (auto-detects Kaggle environment)\n",
    "# Phase 1 + Phase 2 (RandAugment) settings\n",
    "train_config, data_config, system_config = get_config(\n",
    "    num_epochs=70,           # Recommended for Phase 2 (with RandAugment)\n",
    "    batch_size=8,           # Reduced for memory efficiency with freeze_pct=0.0\n",
    "    # All other Phase 1+2 settings are now defaults:\n",
    "    # - freeze_pct=0.0 (train all layers)\n",
    "    # - learning_rate=0.0001 (optimal for freeze_pct=0.0)\n",
    "    # - optimizer=\"adamw\"\n",
    "    # - scheduler=\"cosine\"\n",
    "    # - label_smoothing=0.1 (in model)\n",
    "    # - RandAugment (in datamodule)\n",
    "    # - input_size=384 (memory-efficient)\n",
    "    use_scheduler=True,\n",
    "    scheduler=\"cosine\"\n",
    ")\n",
    "\n",
    "# Optional: Customize early stopping\n",
    "# train_config.early_stop_patience = 10  # More patient\n",
    "# train_config.use_early_stopping = False  # Disable completely\n",
    "\n",
    "print(f\"Training for {train_config.num_epochs} epochs\")\n",
    "print(f\"Model: {train_config.model_name}\")\n",
    "print(f\"Learning rate: {train_config.learning_rate}\")\n",
    "print(f\"Batch size: {train_config.batch_size}\")\n",
    "print(f\"Freeze percentage: {train_config.freeze_pct} (0.0 = all layers trainable)\")\n",
    "print(f\"Optimizer: {train_config.optimizer}\")\n",
    "print(f\"Scheduler: {train_config.scheduler if train_config.use_scheduler else 'none'}\")\n",
    "print(f\"Early stopping: patience={train_config.early_stop_patience}\")\n",
    "print(f\"Device: {system_config.device}\")\n",
    "print(f\"Output directory: {system_config.output_dir}\")\n",
    "print(f\"Batch Size: {train_config.batch_size}\")\n",
    "\n",
    "\n",
    "# Option 1: Use model-specific preprocessing (RECOMMENDED for pre-trained models)\n",
    "# This uses the exact same preprocessing (mean, std, resolution) as the original pre-training\n",
    "print(f\"Using model-specific preprocessing for: {train_config.model_name}\")\n",
    "\n",
    "# Option 2 (Alternative): Calculate from dataset (more accurate for custom stats)\n",
    "# mean, std = calculate_dataset_mean_std(\n",
    "#     annotations_file=data_config.annotations_file,\n",
    "#     img_dir=data_config.img_dir,\n",
    "#     sample_size=None  # Use more samples on Kaggle, None means use all images\n",
    "# )\n",
    "\n",
    "# Option 3 (Alternative): Use pre-computed values (fastest)\n",
    "# mean = [0.5672, 0.4663, 0.3659]\n",
    "# std = [0.2484, 0.2561, 0.2600]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Data Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data module with model-specific preprocessing\n",
    "data_module = KenyanFood13DataModule(\n",
    "    data_config=data_config,\n",
    "    model_name=train_config.model_name\n",
    ")\n",
    "data_module.setup()\n",
    "\n",
    "print(f\"[OK] Data module created with {data_module.num_classes} classes\")\n",
    "\n",
    "# Create model\n",
    "model = KenyanFood13Classifier(train_config, data_module.num_classes)\n",
    "\n",
    "print(f\"[OK] Model created: {train_config.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostics: check labels, mapping, a sample batch and a model forward pass\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- Diagnostics Start ---\")\n",
    "# Check CSV columns and a small preview\n",
    "try:\n",
    "    df = pd.read_csv(data_config.annotations_file)\n",
    "    print(\"CSV columns:\", list(df.columns))\n",
    "    print(df.head())\n",
    "except Exception as e:\n",
    "    print(\"Could not read annotations file:\", e)\n",
    "\n",
    "# Show detected label column and class mapping\n",
    "label_col = getattr(data_module.train_dataset, 'label_col', None)\n",
    "print(\"Detected label column:\", label_col)\n",
    "print(\"class_to_idx mapping (sample):\", dict(list(data_module.train_dataset.class_to_idx.items())[:10]))\n",
    "\n",
    "# Show label distribution (if available)\n",
    "try:\n",
    "    if label_col and label_col in df.columns:\n",
    "        print('\\nLabel distribution (top counts):')\n",
    "        print(df[label_col].value_counts().head(20))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Inspect a single batch from train loader\n",
    "train_loader = data_module.train_dataloader()\n",
    "images, labels = next(iter(train_loader))\n",
    "print('\\nTrain batch images shape:', images.shape)\n",
    "print('Train batch labels shape:', labels.shape)\n",
    "print('Sample label indices:', labels[:10].tolist())\n",
    "\n",
    "# Reverse mapping idx -> class name\n",
    "idx_to_class = {v: k for k, v in data_module.train_dataset.class_to_idx.items()}\n",
    "print('Sample label names:', [idx_to_class.get(int(x), '?') for x in labels[:10]])\n",
    "\n",
    "# Basic image stats (after preprocessing)\n",
    "print('Image min/max:', float(images.min()), float(images.max()))\n",
    "\n",
    "# Quick forward pass through model to inspect outputs\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and system_config.device == 'cuda' else 'cpu')\n",
    "print('Using device for diagnostics:', device)\n",
    "model = model.to(device)\n",
    "images = images.to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(images)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    top1 = probs.argmax(dim=1).cpu().numpy().tolist()\n",
    "    top_conf_vals, _ = probs.max(dim=1)\n",
    "    top_conf = top_conf_vals.cpu().numpy().tolist()\n",
    "    # top_conf = probs.max(dim=1).cpu().numpy().tolist()\n",
    "\n",
    "print('\\nModel predictions (top1 indices):', top1)\n",
    "print('Model top confidences:', [round(float(x), 4) for x in top_conf])\n",
    "\n",
    "print('\\nData module mean/std:', data_module.mean, data_module.std)\n",
    "print('--- Diagnostics End ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n",
    "\n",
    "**Define methods or classes that will be used in model evaluation. For example, accuracy, f1-score etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Updated in trainer.py to include more metrics\n",
    "#we will have methods to calculate accuracy, f1-score, precision, recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n",
    "\n",
    "\n",
    "**Write the methods or classes to be used for training and validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not g_inference_only:\n",
    "    # Train the model\n",
    "    trained_model, _, checkpoint_callback = train_model(\n",
    "        training_config=train_config,\n",
    "        data_config=data_config,\n",
    "        system_config=system_config,\n",
    "        model=model,\n",
    "        data_module=data_module\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[OK] Training complete!\")\n",
    "    print(f\"Best model: {checkpoint_callback.best_model_path}\")\n",
    "    print(f\"Best accuracy: {checkpoint_callback.best_model_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">5. Model [5 Points]</font>\n",
    "\n",
    "**Define your model in this section.**\n",
    "\n",
    "**You are allowed to use any pre-trained model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:  Using EfficientNet V2 S, please refer to ./src/model.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">6. Utils [5 Points]</font>\n",
    "\n",
    "**Define those methods or classes, which have  not been covered in the above sections.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save outputs to Kaggle output folder in a zip for easy download**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not g_inference_only:\n",
    "    import json\n",
    "    import shutil\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Create a clean output directory for the dataset\n",
    "    dataset_dir = Path(\"/kaggle/working/kenyan_food_model_output\")\n",
    "    if dataset_dir.exists():\n",
    "        shutil.rmtree(dataset_dir)\n",
    "    dataset_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Copy the best checkpoint\n",
    "    best_checkpoint = Path(checkpoint_callback.best_model_path)\n",
    "    if best_checkpoint.exists():\n",
    "        shutil.copy(best_checkpoint, dataset_dir / \"best_model.ckpt\")\n",
    "        print(f\"✓ Saved best checkpoint: {best_checkpoint.name}\")\n",
    "\n",
    "    # Save training summary as JSON\n",
    "    summary = {\n",
    "        \"best_val_accuracy\": float(checkpoint_callback.best_model_score),\n",
    "        \"num_epochs\": train_config.num_epochs,\n",
    "        \"batch_size\": train_config.batch_size,\n",
    "        \"learning_rate\": train_config.learning_rate,\n",
    "        \"model\": train_config.model_name,\n",
    "        \"optimizer\": train_config.optimizer,\n",
    "        \"scheduler\": train_config.scheduler if train_config.use_scheduler else \"none\",\n",
    "        \"dataset_mean\": data_module.mean,\n",
    "        \"dataset_std\": data_module.std,\n",
    "        \"num_classes\": data_module.num_classes,\n",
    "        \"checkpoint_path\": str(best_checkpoint.name)\n",
    "    }\n",
    "\n",
    "    with open(dataset_dir / \"training_summary.json\", \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"✓ Saved training summary\")\n",
    "\n",
    "    # Copy ALL TensorBoard logs (full logs for offline review)\n",
    "    tb_logs_src = Path(system_config.output_dir) / \"kenyan_food_logs\"\n",
    "    if tb_logs_src.exists():\n",
    "        tb_logs_dest = dataset_dir / \"tensorboard_logs\"\n",
    "\n",
    "        print(f\"Copying TensorBoard logs from {tb_logs_src}...\")\n",
    "        shutil.copytree(tb_logs_src, tb_logs_dest, dirs_exist_ok=True)\n",
    "\n",
    "        # Count total size of logs for user info\n",
    "        total_size_bytes = sum(f.stat().st_size for f in tb_logs_dest.rglob('*') if f.is_file())\n",
    "        total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "        print(f\"✓ Saved TensorBoard logs ({total_size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"[WARN] TensorBoard logs not found at {tb_logs_src}\")\n",
    "\n",
    "    # Create a ZIP file for easy download\n",
    "    print(\"\\nCreating ZIP archive for download...\")\n",
    "    zip_path = Path(\"/kaggle/working/kenyan_food_model_output\")\n",
    "    shutil.make_archive(str(zip_path), 'zip', dataset_dir)\n",
    "    zip_size_mb = Path(f\"{zip_path}.zip\").stat().st_size / (1024 * 1024)\n",
    "    print(f\"✓ Created kenyan_food_model_output.zip ({zip_size_mb:.1f} MB)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"[OK] OUTPUT READY FOR DOWNLOAD!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nFiles saved to: /kaggle/working/kenyan_food_model_output/\")\n",
    "    print(\"ZIP file: /kaggle/working/kenyan_food_model_output.zip\")\n",
    "    print(\"\\nContents:\")\n",
    "    print(\"  - best_model.ckpt           : Best model checkpoint\")\n",
    "    print(\"  - training_summary.json     : Training metrics and config\")\n",
    "    print(\"  - tensorboard_logs/         : Full TensorBoard event files\")\n",
    "    print(\"\\nTo download:\")\n",
    "    print(\"1. Click 'Output' tab in the right sidebar\")\n",
    "    print(\"2. Find 'kenyan_food_model_output.zip'\")\n",
    "    print(\"3. Click the download button\")\n",
    "    print(\"\\nOr use Kaggle API to create a dataset for reuse in other notebooks!\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.a: Generate Test Predictions\n",
    "\n",
    "After training completes, generate predictions on test data and create submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference import create_submission\n",
    "if g_inference_only:\n",
    "    checkpoint_path = \"/kaggle/working/kenyan_food_model_output/best_model.ckpt\"\n",
    "else:\n",
    "    checkpoint_path = checkpoint_callback.best_model_path\n",
    "\n",
    "# Generate predictions and create submission.csv\n",
    "# Automatically detects Kaggle environment and uses correct paths\n",
    "submission_df = create_submission(\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    output_csv_path=\"/kaggle/working/submission.csv\",\n",
    "    model_config=train_config,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "print(f\"[OK] Submission created: /kaggle/working/submission.csv\")\n",
    "print(f\"[INFO] Total predictions: {len(submission_df)}\")\n",
    "print(f\"\\n[INFO] Prediction distribution:\")\n",
    "print(submission_df['label'].value_counts())\n",
    "print(\"\\n[OK] Ready to download and submit to Kaggle!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">7. Experiment [5 Points]</font>\n",
    "\n",
    "**Choose your optimizer and LR-scheduler and use the above methods and classes to train your model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Updated in src/trainer.py module.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">8. TensorBoard Log Link [5 Points]</font>\n",
    "\n",
    "**Share your TensorBoard scalars logs link here You can also share (not mandatory) your GitHub link, if you have pushed this project in GitHub.**\n",
    "\n",
    "\n",
    "Note: In light of the recent shutdown of tensorboard.dev, we have updated the submission requirements for your project. Instead of sharing a tensorboard.dev link, you are now required to upload your generated TensorBoard event files directly onto the lab. As an alternative, you may also include a screenshot of your TensorBoard output within your Jupyter notebook. This adjustment ensures that your data visualization and model training efforts are thoroughly documented and accessible for evaluation.\n",
    "\n",
    "You are also welcome (and encouraged) to utilize alternative logging services like wandB or comet. In such instances, you can easily make your project logs publicly accessible and share the link with others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Will upload the logs along with this code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n",
    "\n",
    "**Share your Kaggle profile link  with us here to score , points in  the competition.**\n",
    "\n",
    "**For full points, you need a minimum accuracy of `75%` on the test data. If accuracy is less than `70%`, you gain  no points for this section.**\n",
    "\n",
    "\n",
    "**Submit `submission.csv` (prediction for images in `test.csv`), in the `Submit Predictions` tab in Kaggle, to get evaluated for  this section.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Rama Kaggle Profile](https://www.kaggle.com/ramabyg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10665762,
     "sourceId": 90936,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
