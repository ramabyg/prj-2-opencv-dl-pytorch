{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "### <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n",
    "\n",
    "#### Maximum Points: 100\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n",
    "        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Dev Scalars Log Link</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n",
    "    </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n",
    "\n",
    "In this section, you have to write a class or methods, which will be used to get training and validation data loader.\n",
    "\n",
    "You need to write a custom dataset class to load data.\n",
    "\n",
    "**Note; There is   no separate validation data. , You will thus have to create your own validation set, by dividing the train data into train and validation data. Usually, we do 80:20 ratio for train and validation, respectively.**\n",
    "\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "class KenyanFood13Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "    ....\n",
    "    ...\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "def get_data(args1, *args):\n",
    "    ....\n",
    "    ....\n",
    "    return train_loader, test_loader\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "#global flag to indicate if the script is running in a local environment\n",
    "g_local_run: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from torchmetrics.classification import  MulticlassAccuracy, MulticlassF1Score, MulticlassPrecision, MulticlassRecall\n",
    "from torchmetrics import MeanMetric\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class KenyanFood13Dataset(Dataset):\n",
    "\n",
    "    \"\"\"Custom Dataset for Kenyan Food 13 Classification Task\"\"\"\n",
    "    \"\"\" Accepts a CSV file with image ID and lable colums,\n",
    "    Will split total images into train and validation sets with 80:20 ratio\n",
    "    First 80% images will be used for training and remaining 20% for validation\n",
    "    Args:\n",
    "        annotations_file (string): Path to the csv file with annotations.\n",
    "        img_dir (string): Directory with all the images.\n",
    "        train (bool, optional): Indicates if the dataset is for training or validation.\n",
    "            Default is True for training set, False for validation set.\n",
    "        transform (callable, optional): Optional transform to be applied\n",
    "            on a sample.\n",
    "        target_transform (callable, optional): Optional transform to be applied\n",
    "    \"\"\"\n",
    "    def __init__(self, annotations_file, img_dir, train=True, transform=None, target_transform=None):\n",
    "\n",
    "        if g_local_run:\n",
    "            print(\"Running in local mode - loading data from local paths\")\n",
    "            #add 'local' to annotatins_file name\n",
    "            base, ext = os.path.splitext(annotations_file)\n",
    "            annotations_file = f\"{base}_local{ext}\"\n",
    "\n",
    "        # few error checks\n",
    "        if not os.path.exists(annotations_file):\n",
    "            raise FileNotFoundError(f\"Annotations file not found: {annotations_file}\")\n",
    "        if not os.path.exists(img_dir):\n",
    "            raise FileNotFoundError(f\"Image directory not found: {img_dir}\")\n",
    "\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.local_run = g_local_run\n",
    "\n",
    "        num_classes = len(self.img_labels['label'].unique())\n",
    "        self.num_classes = num_classes\n",
    "        print(f\"Dataset initialized with {len(self.img_labels)} samples belonging to {num_classes} classes.\")\n",
    "        #split into train and validation sets\n",
    "        split_index = int(0.8 * len(self.img_labels))\n",
    "        if train:\n",
    "            self.img_labels = self.img_labels.iloc[:split_index].reset_index(drop=True)\n",
    "            print(f\"Using {len(self.img_labels)} samples for training.\")\n",
    "        else:\n",
    "            self.img_labels = self.img_labels.iloc[split_index:].reset_index(drop=True)\n",
    "            print(f\"Using {len(self.img_labels)} samples for validation.\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2. Configuration [5 Points]</font>\n",
    "\n",
    "**Define your configuration here.**\n",
    "\n",
    "For example:\n",
    "\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 10 \n",
    "    epochs_count: int = 50  \n",
    "    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n",
    "    log_interval: int = 5  \n",
    "    test_interval: int = 1  \n",
    "    data_root: str = \"/kaggle/input/opencv-pytorch-project-2-classification-round-3\" \n",
    "    num_workers: int = 2  \n",
    "    device: str = 'cuda'  \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# configurations\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 0.001\n",
    "    num_epochs: int = 10\n",
    "    momentum: float = 0.9\n",
    "    log_interval: int = 10\n",
    "    random_seed: int = 42\n",
    "\n",
    "    model_name: str = \"googlenet\" # base model we will use for transfer learning and fine-tuning\n",
    "    pretrained: bool = True # use pretrained weights for the base model\n",
    "    precision: str = \"float32\" # precision for training: float32, float16, bfloat16\n",
    "    fine_tune_start: int = 5 # layer from which to start fine-tuning (1 means all layers, higher means fewer layers)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataConfiguration:\n",
    "    if g_local_run\n",
    "        annotations_file: str = \"../data/kenyan-food-13/train.csv\"\n",
    "        img_dir: str = \"../data/kenyan-food-13/images/images\"\n",
    "    else:\n",
    "        annotations_file: str = \"/kaggle/input/kenyan-food-13/train.csv\"\n",
    "        img_dir: str = \"/kaggle/input/kenyan-food-13/images/images\"\n",
    "\n",
    "    input_size: int = 224 # input image size for the model\n",
    "    num_workers: int = 4 # number of workers for data loading\n",
    "\n",
    "@dataclass\n",
    "class systemConfiguration:\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if g_local_run:\n",
    "        output_dir: str = \"./output\" # directory to save model checkpoints and logs\n",
    "    else:\n",
    "        output_dir: str = \"/kaggle/working/output\" # directory to save model checkpoints and logs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_config = DataConfiguration()\n",
    "train_config = TrainingConfiguration()\n",
    "system_config = systemConfiguration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n",
    "\n",
    "**Define methods or classes that will be used in model evaluation. For example, accuracy, f1-score etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#we will have methods to calculate accuracy, f1-score, precision, recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n",
    "\n",
    "\n",
    "**Write the methods or classes to be used for training and validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def training_validation(training_config: TrainingConfiguration,\n",
    "                        data_config: DataConfiguration,\n",
    "                        system_config: systemConfiguration):\n",
    "\n",
    "    #random seed for reproducibility\n",
    "    pl.seed_everything(training_config.random_seed)\n",
    "\n",
    "    model = KenyanFood13Classifier(training_config, data_config.num_classes)\n",
    "    data_module = ... # define your data module here\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=system_config.output_dir,\n",
    "        filename=\"{epoch}-{val_loss:.2f}\",\n",
    "        save_top_k=3,\n",
    "        monitor=\"valid/acc\",\n",
    "        mode=\"max\",\n",
    "        auto_insert_metric_name=False,\n",
    "        save_weights_only=True)\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor=\"valid/acc\",\n",
    "        patience=3,\n",
    "        mode=\"max\")\n",
    "\n",
    "    # Map precision string to PyTorch Lightning expected value\n",
    "    precision_map = {\n",
    "        \"float32\": 32,\n",
    "        \"float16\": 16,\n",
    "        \"bfloat16\": \"bf16\"\n",
    "    }\n",
    "    trainer_precision = precision_map.get(training_config.precision, 32)\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=training_config.num_epochs,\n",
    "        accelerator=system_config.device,\n",
    "        devices=\"auto\",\n",
    "        precision=trainer_precision,\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "        default_root_dir=system_config.output_dir,\n",
    "        log_every_n_steps=training_config.log_interval\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "    trainer.validate(model, datamodule=data_module)\n",
    "\n",
    "    return model, data_module, checkpoint_callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">5. Model [5 Points]</font>\n",
    "\n",
    "**Define your model in this section.**\n",
    "\n",
    "**You are allowed to use any pre-trained model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LightningModule, we will use GoogleNet as base model for transfer learning and fine-tuning.\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class KenyanFood13Classifier(L.LightningModule):\n",
    "    def __init__(self, training_config: TrainingConfiguration, num_classes: int):\n",
    "        super(KenyanFood13Classifier, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Load base model\n",
    "        if training_config.model_name == \"googlenet\":\n",
    "            self.model = torchvision.models.googlenet(pretrained=training_config.pretrained)\n",
    "            # Replace the final layer\n",
    "            self.model.fc = torch.nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        else:\n",
    "            raise ValueError(f\"Model {training_config.model_name} not supported.\")\n",
    "\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.train_mean_loss = MeanMetric()\n",
    "        self.val_mean_loss = MeanMetric()\n",
    "\n",
    "\n",
    "        self.train_accuracy = MulticlassAccuracy(num_classes=num_classes, average='macro')\n",
    "        self.val_accuracy = MulticlassAccuracy(num_classes=num_classes, average='macro')\n",
    "        self.train_f1 = MulticlassF1Score(num_classes=num_classes, average='macro')\n",
    "        self.val_f1 = MulticlassF1Score(num_classes=num_classes, average='macro')\n",
    "        self.train_precision = MulticlassPrecision(num_classes=num_classes, average='macro')\n",
    "        self.val_precision = MulticlassPrecision(num_classes=num_classes, average='macro')\n",
    "        self.train_recall = MulticlassRecall(num_classes=num_classes, average='macro')\n",
    "        self.val_recall = MulticlassRecall(num_classes=num_classes, average='macro')\n",
    "\n",
    "        self.learning_rate = training_config.learning_rate\n",
    "        self.momentum = training_config.momentum\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # get data from batch images, labels\n",
    "        images, labels = batch\n",
    "        # predictions\n",
    "        outputs = self(images)\n",
    "        # calculate loss, uses cross-entropy loss\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        self.train_mean_loss.update(loss)\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        self.train_accuracy.update(preds, labels)\n",
    "        self.train_mean_loss.update(loss)\n",
    "        self.train_precision.update(preds, labels)\n",
    "        self.train_recall.update(preds, labels)\n",
    "        self.train_f1.update(preds, labels)\n",
    "        self.log('train/loss', self.train_mean_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train/acc', self.train_accuracy, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        #update  epoch level metrics and reset\n",
    "        self.log('train/precision', self.train_precision.compute(), on_epoch=True, prog_bar=True)\n",
    "        self.log('train/recall', self.train_recall.compute(), on_epoch=True, prog_bar=True)\n",
    "        self.log('train/f1', self.train_f1.compute(), on_epoch=True, prog_bar=True)\n",
    "        self.log('step', self.current_epoch, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return super().on_train_epoch_end()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # get data from batch images, labels\n",
    "        images, labels = batch\n",
    "        # predictions\n",
    "        outputs = self(images)\n",
    "        # calculate loss, uses cross-entropy loss\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        self.val_mean_loss.update(loss)\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        self.val_accuracy.update(preds, labels)\n",
    "        self.log('valid/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('valid/acc', self.val_accuracy, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        #update  epoch level metrics and reset\n",
    "        self.log('valid/precision', self.val_precision.compute(), on_epoch=True, prog_bar=True)\n",
    "        self.log('valid/recall', self.val_recall.compute(), on_epoch=True, prog_bar=True)\n",
    "        self.log('valid/f1', self.val_f1.compute(), on_epoch=True, prog_bar=True)\n",
    "        self.log('step', self.current_epoch, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return super().on_validation_epoch_end()\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate, momentum=self.momentum)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">6. Utils [5 Points]</font>\n",
    "\n",
    "**Define those methods or classes, which have  not been covered in the above sections.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">7. Experiment [5 Points]</font>\n",
    "\n",
    "**Choose your optimizer and LR-scheduler and use the above methods and classes to train your model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">8. TensorBoard Log Link [5 Points]</font>\n",
    "\n",
    "**Share your TensorBoard scalars logs link here You can also share (not mandatory) your GitHub link, if you have pushed this project in GitHub.**\n",
    "\n",
    "\n",
    "Note: In light of the recent shutdown of tensorboard.dev, we have updated the submission requirements for your project. Instead of sharing a tensorboard.dev link, you are now required to upload your generated TensorBoard event files directly onto the lab. As an alternative, you may also include a screenshot of your TensorBoard output within your Jupyter notebook. This adjustment ensures that your data visualization and model training efforts are thoroughly documented and accessible for evaluation.\n",
    "\n",
    "You are also welcome (and encouraged) to utilize alternative logging services like wandB or comet. In such instances, you can easily make your project logs publicly accessible and share the link with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n",
    "\n",
    "**Share your Kaggle profile link  with us here to score , points in  the competition.**\n",
    "\n",
    "**For full points, you need a minimum accuracy of `75%` on the test data. If accuracy is less than `70%`, you gain  no points for this section.**\n",
    "\n",
    "\n",
    "**Submit `submission.csv` (prediction for images in `test.csv`), in the `Submit Predictions` tab in Kaggle, to get evaluated for  this section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10665762,
     "sourceId": 90936,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
