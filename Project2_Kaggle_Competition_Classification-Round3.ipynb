{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "### <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n",
    "\n",
    "#### Maximum Points: 100\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n",
    "        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Dev Scalars Log Link</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n",
    "    </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n",
    "\n",
    "In this section, you have to write a class or methods, which will be used to get training and validation data loader.\n",
    "\n",
    "You need to write a custom dataset class to load data.\n",
    "\n",
    "**Note; There is   no separate validation data. , You will thus have to create your own validation set, by dividing the train data into train and validation data. Usually, we do 80:20 ratio for train and validation, respectively.**\n",
    "\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "class KenyanFood13Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "    ....\n",
    "    ...\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "def get_data(args1, *args):\n",
    "    ....\n",
    "    ....\n",
    "    return train_loader, test_loader\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightning in /usr/local/lib/python3.10/dist-packages (2.6.0)\n",
      "Requirement already satisfied: PyYAML<8.0,>5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.2)\n",
      "Requirement already satisfied: PyYAML<8.0,>5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.2)\n",
      "Requirement already satisfied: fsspec<2027.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (2024.6.1)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.11.9)\n",
      "Requirement already satisfied: packaging<27.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.1)\n",
      "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.4.1+cu121)\n",
      "Requirement already satisfied: torchmetrics<3.0,>0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.6.0)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<6.0,>4.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.4.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (3.10.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (71.0.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.16.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.3)\n",
      "Requirement already satisfied: fsspec<2027.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (2024.6.1)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.11.9)\n",
      "Requirement already satisfied: packaging<27.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.1)\n",
      "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.4.1+cu121)\n",
      "Requirement already satisfied: torchmetrics<3.0,>0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.6.0)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<6.0,>4.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.4.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (3.10.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (71.0.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.16.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\n",
      "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics<3.0,>0.7.0->lightning) (1.26.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\n",
      "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics<3.0,>0.7.0->lightning) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.11.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.11.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (3.10)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (3.10)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "#global flag to indicate if the script is running in a local environment\n",
    "g_local_run: bool = False\n",
    "g_measure_mean_std = True\n",
    "\n",
    "if not g_local_run:\n",
    "    !pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from torchmetrics.classification import  MulticlassAccuracy, MulticlassF1Score, MulticlassPrecision, MulticlassRecall\n",
    "from torchmetrics import MeanMetric\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if g_local_run:\n",
    "    from kenyan_data import KenyanFood13Dataset, KenyanFood13DataModule\n",
    "else:\n",
    "\n",
    "    class KenyanFood13Dataset(Dataset):\n",
    "        \"\"\"Top-level dataset module for pickling across DataLoader workers.\n",
    "\n",
    "        Notes:\n",
    "        - Define this class in a standalone module (not in a notebook cell) so\n",
    "        multiprocessing (spawn) on Windows can import it in worker processes.\n",
    "        - The CSV is expected to contain at least two columns: an image id/filename\n",
    "        and a class label. By default this implementation uses the first two\n",
    "        columns, but it's safer to pass explicit column names (see args).\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self,\n",
    "                    annotations_file: str,\n",
    "                    img_dir: str,\n",
    "                    train: bool = True,\n",
    "                    transform: Optional[transforms.Compose] = None,\n",
    "                    target_transform: Optional[callable] = None,\n",
    "                    id_col: Optional[str] = None,\n",
    "                    class_col: Optional[str] = None):\n",
    "\n",
    "            annotations_file = os.path.abspath(annotations_file)\n",
    "            img_dir = os.path.abspath(img_dir)\n",
    "\n",
    "            if not os.path.exists(annotations_file):\n",
    "                raise FileNotFoundError(f\"Annotations file not found: {annotations_file}\")\n",
    "            if not os.path.exists(img_dir):\n",
    "                raise FileNotFoundError(f\"Image directory not found: {img_dir}\")\n",
    "\n",
    "            self.df = pd.read_csv(annotations_file)\n",
    "            if self.df.shape[1] < 2:\n",
    "                raise ValueError(\"Annotations CSV must contain at least two columns: id and class\")\n",
    "\n",
    "            # determine columns\n",
    "            if id_col is None:\n",
    "                id_col = self.df.columns[0]\n",
    "            if class_col is None:\n",
    "                class_col = self.df.columns[1]\n",
    "\n",
    "            self.id_col = id_col\n",
    "            self.class_col = class_col\n",
    "\n",
    "            # ensure strings for ids and classes\n",
    "            self.df[self.id_col] = self.df[self.id_col].astype(str)\n",
    "            self.df[self.class_col] = self.df[self.class_col].astype(str)\n",
    "\n",
    "            # build label mapping using the full CSV so mapping is consistent\n",
    "            unique_labels = self.df[self.class_col].unique()\n",
    "            self.label2idx = {label: int(idx) for idx, label in enumerate(sorted(unique_labels))}\n",
    "            self.idx2label = {int(idx): label for label, idx in self.label2idx.items()}\n",
    "\n",
    "            self.img_dir = img_dir\n",
    "            self.transform = transform\n",
    "            self.target_transform = target_transform\n",
    "            self.train = train\n",
    "\n",
    "            # split dataset (80:20)\n",
    "            total = len(self.df)\n",
    "            split_index = int(0.8 * total)\n",
    "            if self.train:\n",
    "                self.df = self.df.iloc[:split_index].reset_index(drop=True)\n",
    "            else:\n",
    "                self.df = self.df.iloc[split_index:].reset_index(drop=True)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.df)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            if torch.is_tensor(idx):\n",
    "                idx = idx.tolist()\n",
    "\n",
    "            row = self.df.iloc[idx]\n",
    "            img_id = str(row[self.id_col])\n",
    "            img_path = os.path.join(self.img_dir, img_id if img_id.lower().endswith(('.jpg', '.jpeg', '.png')) else img_id + '.jpg')\n",
    "\n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to open image {img_path}: {e}\")\n",
    "\n",
    "            label_str = str(row[self.class_col])\n",
    "            label = self.label2idx.get(label_str)\n",
    "            if label is None:\n",
    "                raise KeyError(f\"Label '{label_str}' not found in label2idx mapping\")\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            if self.target_transform:\n",
    "                label = self.target_transform(label)\n",
    "\n",
    "            return image, int(label)\n",
    "\n",
    "    class KenyanFood13DataModule(L.LightningDataModule):\n",
    "        \"\"\"Lightning DataModule for Kenyan Food 13 Dataset.\n",
    "\n",
    "        This properly inherits from LightningDataModule so it works with Lightning Trainer.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, data_config, mean=None, std=None):\n",
    "            super().__init__()\n",
    "            self.data_config = data_config\n",
    "            self.mean = mean if mean is not None else [0.485, 0.456, 0.406]\n",
    "            self.std = std if std is not None else [0.229, 0.224, 0.225]\n",
    "            self.train_dataset = None\n",
    "            self.val_dataset = None\n",
    "\n",
    "        def prepare_data(self):\n",
    "            # no-op: data is expected to be available locally; override if downloading is needed\n",
    "            return None\n",
    "\n",
    "        def setup(self, stage: Optional[str] = None):\n",
    "            # Lightning calls setup(stage=...) so accept the optional stage argument\n",
    "            if isinstance(self.data_config.input_size, int):\n",
    "                h = w = self.data_config.input_size\n",
    "            else:\n",
    "                h, w = self.data_config.input_size\n",
    "\n",
    "            common_transforms = transforms.Compose([\n",
    "                transforms.Resize((h, w)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=self.mean, std=self.std),\n",
    "            ])\n",
    "\n",
    "            aug_transforms = transforms.Compose([\n",
    "                transforms.RandomResizedCrop((h, w)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=self.mean, std=self.std),\n",
    "            ])\n",
    "\n",
    "            self.train_dataset = KenyanFood13Dataset(\n",
    "                annotations_file=self.data_config.annotations_file,\n",
    "                img_dir=self.data_config.img_dir,\n",
    "                train=True,\n",
    "                transform=aug_transforms\n",
    "            )\n",
    "\n",
    "            self.val_dataset = KenyanFood13Dataset(\n",
    "                annotations_file=self.data_config.annotations_file,\n",
    "                img_dir=self.data_config.img_dir,\n",
    "                train=False,\n",
    "                transform=common_transforms\n",
    "            )\n",
    "\n",
    "            self.num_classes = len(self.train_dataset.label2idx)\n",
    "\n",
    "        def train_dataloader(self):\n",
    "            if self.train_dataset is None:\n",
    "                raise RuntimeError('Call setup() before train_dataloader()')\n",
    "            return DataLoader(self.train_dataset,\n",
    "                    batch_size=self.data_config.batch_size,\n",
    "                    shuffle=True,\n",
    "                    num_workers=self.data_config.num_workers,\n",
    "                    pin_memory=True if torch.cuda.is_available() else False,\n",
    "                    persistent_workers=True if self.data_config.num_workers > 0 else False)\n",
    "\n",
    "        def val_dataloader(self):\n",
    "            if self.val_dataset is None:\n",
    "                raise RuntimeError('Call setup() before val_dataloader()')\n",
    "            return DataLoader(self.val_dataset,\n",
    "                    batch_size=self.data_config.batch_size,\n",
    "                    shuffle=False,\n",
    "                    num_workers=self.data_config.num_workers,\n",
    "                    pin_memory=True if torch.cuda.is_available() else False,\n",
    "                    persistent_workers=True if self.data_config.num_workers > 0 else False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataloader_test.py\n",
    "# # from kenyan_data import KenyanFood13DataModule\n",
    "\n",
    "# # Minimal DataConfiguration compatible with your DataConfiguration dataclass in the notebook.\n",
    "# @dataclass\n",
    "# class DummyDataConfig:\n",
    "#     annotations_file: str = \"../../data/kenyan-food-13/train_local.csv\"\n",
    "#     img_dir: str = \"../../data/kenyan-food-13/images/images\"\n",
    "#     input_size: int = 224\n",
    "#     num_workers: int = 4\n",
    "#     batch_size: int = 32\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     def inspect_loader(loader, name=\"loader\"):\n",
    "#         it = iter(loader)\n",
    "#         batch = next(it)\n",
    "#         images, labels = batch\n",
    "#         print(f\"{name} - images.shape: {images.shape}, labels.shape: {labels.shape}\")\n",
    "#         try:\n",
    "#             unique, counts = torch.unique(labels, return_counts=True)\n",
    "#             print(f\"{name} - label counts in batch: {dict(zip(unique.tolist(), counts.tolist()))}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Could not compute label counts: {e}\")\n",
    "\n",
    "#     cfg = DummyDataConfig()\n",
    "#     dm = KenyanFood13DataModule(data_config=cfg)\n",
    "#     ds = dm.train_dataset\n",
    "#     print(\"dataset class module:\", ds.__class__.__module__)\n",
    "#     print(\"dataset class qualname:\", ds.__class__.__qualname__)\n",
    "#     try:\n",
    "#         dm.setup()\n",
    "#         tr_dl = dm.train_dataloader()\n",
    "#         vl_dl = dm.val_dataloader()\n",
    "#         inspect_loader(tr_dl, 'train')\n",
    "#         inspect_loader(vl_dl, 'val')\n",
    "\n",
    "#     except Exception:\n",
    "#         traceback.print_exc()\n",
    "\n",
    "# TEST: DataLoader with num_workers from notebook-defined class\n",
    "# This test will FAIL with num_workers > 0 because notebook classes aren't picklable\n",
    "\n",
    "# from kenyan_data import KenyanFood13DataModule\n",
    "\n",
    "# @dataclass\n",
    "# class DummyDataConfig:\n",
    "#     annotations_file: str = \"../../data/kenyan-food-13/train_local.csv\"\n",
    "#     img_dir: str = \"../../data/kenyan-food-13/images/images\"\n",
    "#     input_size: int = 224\n",
    "#     num_workers: int = 4  # IMPORTANT: Keep 0 for notebook classes, use >0 only with kenyan_data.py import\n",
    "#     batch_size: int = 32\n",
    "\n",
    "# def inspect_loader(loader, name=\"loader\"):\n",
    "#     it = iter(loader)\n",
    "#     batch = next(it)\n",
    "#     images, labels = batch\n",
    "#     print(f\"{name} - images.shape: {images.shape}, labels.shape: {labels.shape}\")\n",
    "#     try:\n",
    "#         unique, counts = torch.unique(labels, return_counts=True)\n",
    "#         print(f\"{name} - label counts in batch: {dict(zip(unique.tolist(), counts.tolist()))}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Could not compute label counts: {e}\")\n",
    "\n",
    "# cfg = DummyDataConfig()\n",
    "# dm = KenyanFood13DataModule(data_config=cfg)\n",
    "# try:\n",
    "#     dm.setup()\n",
    "#     print(\"\\nðŸ“Œ IMPORTANT:\")\n",
    "#     print(\"  - Dataset defined in notebook: __module__ =\", dm.train_dataset.__class__.__module__)\n",
    "#     print(\"  - With num_workers > 0, multiprocessing WILL FAIL (can't pickle notebook classes)\")\n",
    "#     print(\"  - To use num_workers > 0, use: from kenyan_data import KenyanFood13Dataset\")\n",
    "#     print(\"\\nâœ“ Testing with num_workers=0:\")\n",
    "#     tr_dl = dm.train_dataloader()\n",
    "#     vl_dl = dm.val_dataloader()\n",
    "#     inspect_loader(tr_dl, 'train')\n",
    "#     inspect_loader(vl_dl, 'val')\n",
    "#     print(\"\\nâœ“ SUCCESS with num_workers=0\")\n",
    "# except Exception as e:\n",
    "#     print(\"\\nâœ— FAILED:\")\n",
    "#     traceback.print_exc()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2. Configuration [5 Points]</font>\n",
    "\n",
    "**Define your configuration here.**\n",
    "\n",
    "For example:\n",
    "\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 10 \n",
    "    epochs_count: int = 50  \n",
    "    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n",
    "    log_interval: int = 5  \n",
    "    test_interval: int = 1  \n",
    "    data_root: str = \"/kaggle/input/opencv-pytorch-project-2-classification-round-3\" \n",
    "    num_workers: int = 2  \n",
    "    device: str = 'cuda'  \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# configurations\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 0.001\n",
    "    num_epochs: int = 10\n",
    "    momentum: float = 0.9\n",
    "    log_interval: int = 5\n",
    "    random_seed: int = 42\n",
    "\n",
    "    # Optimizer configuration\n",
    "    optimizer: str = \"sgd\"  # Options: 'sgd', 'adam', 'adamw'\n",
    "    weight_decay: float = 0.0001  # L2 regularization\n",
    "\n",
    "    # Learning rate scheduler configuration\n",
    "    use_scheduler: bool = True\n",
    "    scheduler: str = \"step\"  # Options: 'step', 'cosine', 'reduce_on_plateau'\n",
    "    lr_step_size: int = 5  # For StepLR: step size for learning rate decay\n",
    "    lr_gamma: float = 0.1  # For StepLR: multiplicative factor of learning rate decay\n",
    "\n",
    "    model_name: str = \"googlenet\" # base model we will use for transfer learning and fine-tuning\n",
    "    pretrained: bool = True # use pretrained weights for the base model\n",
    "    precision: str = \"float32\" # precision for training: float32, float16, bfloat16\n",
    "    fine_tune_start: int = 5 # layer from which to start fine-tuning (1 means all layers, higher means fewer layers)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataConfiguration:\n",
    "    if g_local_run:\n",
    "        print(\"Running in local mode - loading data from local paths\")\n",
    "    annotations_file: str = \"../../data/kenyan-food-13/train_local.csv\" if g_local_run else \"/kaggle/input/opencv-pytorch-project-2-classification-round-3/train.csv\"\n",
    "    img_dir: str = \"../../data/kenyan-food-13/images/images\" if g_local_run else \"/kaggle/input/opencv-pytorch-project-2-classification-round-3/images/images\"\n",
    "    #check if annotations file and data exist\n",
    "    try:\n",
    "        annotations_file = os.path.abspath(annotations_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error resolving path for annotations_file: {e}\")\n",
    "        exit(1)\n",
    "    try:\n",
    "        img_dir = os.path.abspath(img_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error resolving path for img_dir: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "    input_size: int = 224 # input image size for the model\n",
    "    num_workers: int = 4 # number of workers for data loading\n",
    "    batch_size: int = 32 # batch size for training and validation\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SystemConfiguration:\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    output_dir: str = \"./output\" if g_local_run else \"/kaggle/working/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_config = DataConfiguration()\n",
    "train_config = TrainingConfiguration()\n",
    "system_config = SystemConfiguration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n",
    "\n",
    "**Define methods or classes that will be used in model evaluation. For example, accuracy, f1-score etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#we will have methods to calculate accuracy, f1-score, precision, recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LightningModule, we will use GoogleNet as base model for transfer learning and fine-tuning.\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class KenyanFood13Classifier(L.LightningModule):\n",
    "    def __init__(self, training_config: TrainingConfiguration, num_classes: int):\n",
    "        super(KenyanFood13Classifier, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Store training configuration\n",
    "        self.training_config = training_config\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Load base model\n",
    "        if training_config.model_name == \"googlenet\":\n",
    "            if train_config.pretrained:\n",
    "                weights = torchvision.models.GoogLeNet_Weights.IMAGENET1K_V1\n",
    "            else:\n",
    "                weights = None\n",
    "            self.model = torchvision.models.googlenet(weights=weights)\n",
    "            # Replace the final layer\n",
    "            self.model.fc = torch.nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        else:\n",
    "            raise ValueError(f\"Model {training_config.model_name} not supported.\")\n",
    "\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.train_mean_loss = MeanMetric()\n",
    "        self.val_mean_loss = MeanMetric()\n",
    "\n",
    "        self.train_accuracy = MulticlassAccuracy(num_classes=num_classes, average='macro')\n",
    "        self.val_accuracy = MulticlassAccuracy(num_classes=num_classes, average='macro')\n",
    "        self.train_f1 = MulticlassF1Score(num_classes=num_classes, average='macro')\n",
    "        self.val_f1 = MulticlassF1Score(num_classes=num_classes, average='macro')\n",
    "        self.train_precision = MulticlassPrecision(num_classes=num_classes, average='macro')\n",
    "        self.val_precision = MulticlassPrecision(num_classes=num_classes, average='macro')\n",
    "        self.train_recall = MulticlassRecall(num_classes=num_classes, average='macro')\n",
    "        self.val_recall = MulticlassRecall(num_classes=num_classes, average='macro')\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # get data from batch images, labels\n",
    "        images, labels = batch\n",
    "        # predictions\n",
    "        outputs = self(images)\n",
    "        # calculate loss, uses cross-entropy loss\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        self.train_mean_loss.update(loss)\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        self.train_accuracy.update(preds, labels)\n",
    "        self.train_mean_loss.update(loss)\n",
    "        self.train_precision.update(preds, labels)\n",
    "        self.train_recall.update(preds, labels)\n",
    "        self.train_f1.update(preds, labels)\n",
    "        self.log('train/loss', self.train_mean_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train/acc', self.train_accuracy, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        #update  epoch level metrics and reset\n",
    "        self.log('train/precision', self.train_precision.compute(), on_epoch=True, prog_bar=True)\n",
    "        self.log('train/recall', self.train_recall.compute(), on_epoch=True, prog_bar=True)\n",
    "        self.log('train/f1', self.train_f1.compute(), on_epoch=True, prog_bar=True)\n",
    "        self.log('step', self.current_epoch, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return super().on_train_epoch_end()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # get data from batch images, labels\n",
    "        images, labels = batch\n",
    "        # predictions\n",
    "        outputs = self(images)\n",
    "        # calculate loss, uses cross-entropy loss\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        self.val_mean_loss.update(loss)\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        self.val_accuracy.update(preds, labels)\n",
    "        self.val_precision.update(preds, labels)\n",
    "        self.val_recall.update(preds, labels)\n",
    "        self.val_f1.update(preds, labels)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('valid/loss', self.val_mean_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('valid/acc', self.val_accuracy, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('valid/precision', self.val_precision, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log('valid/recall', self.val_recall, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log('valid/f1', self.val_f1, on_step=False, on_epoch=True, prog_bar=False)\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        #update  epoch level metrics and reset\n",
    "        self.log('valid/precision', self.val_precision.compute(), on_epoch=True, prog_bar=True)\n",
    "        self.log('valid/recall', self.val_recall.compute(), on_epoch=True, prog_bar=True)\n",
    "        self.log('valid/f1', self.val_f1.compute(), on_epoch=True, prog_bar=True)\n",
    "        self.log('step', self.current_epoch, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return super().on_validation_epoch_end()\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Create optimizer based on configuration\n",
    "        if self.training_config.optimizer.lower() == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(\n",
    "                self.parameters(),\n",
    "                lr=self.training_config.learning_rate,\n",
    "                momentum=self.training_config.momentum,\n",
    "                weight_decay=self.training_config.weight_decay\n",
    "            )\n",
    "        elif self.training_config.optimizer.lower() == \"adam\":\n",
    "            optimizer = torch.optim.Adam(\n",
    "                self.parameters(),\n",
    "                lr=self.training_config.learning_rate,\n",
    "                weight_decay=self.training_config.weight_decay\n",
    "            )\n",
    "        elif self.training_config.optimizer.lower() == \"adamw\":\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                self.parameters(),\n",
    "                lr=self.training_config.learning_rate,\n",
    "                weight_decay=self.training_config.weight_decay\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Optimizer {self.training_config.optimizer} not supported.\")\n",
    "\n",
    "        # Configure learning rate scheduler if enabled\n",
    "        if self.training_config.use_scheduler:\n",
    "            if self.training_config.scheduler.lower() == \"step\":\n",
    "                scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "                    optimizer,\n",
    "                    step_size=self.training_config.lr_step_size,\n",
    "                    gamma=self.training_config.lr_gamma\n",
    "                )\n",
    "                return {\n",
    "                    \"optimizer\": optimizer,\n",
    "                    \"lr_scheduler\": {\n",
    "                        \"scheduler\": scheduler,\n",
    "                        \"interval\": \"epoch\",\n",
    "                        \"frequency\": 1\n",
    "                    }\n",
    "                }\n",
    "            elif self.training_config.scheduler.lower() == \"cosine\":\n",
    "                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                    optimizer,\n",
    "                    T_max=self.training_config.num_epochs\n",
    "                )\n",
    "                return {\n",
    "                    \"optimizer\": optimizer,\n",
    "                    \"lr_scheduler\": {\n",
    "                        \"scheduler\": scheduler,\n",
    "                        \"interval\": \"epoch\",\n",
    "                        \"frequency\": 1\n",
    "                    }\n",
    "                }\n",
    "            elif self.training_config.scheduler.lower() == \"reduce_on_plateau\":\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer,\n",
    "                    mode='max',\n",
    "                    factor=self.training_config.lr_gamma,\n",
    "                    patience=3\n",
    "                )\n",
    "                return {\n",
    "                    \"optimizer\": optimizer,\n",
    "                    \"lr_scheduler\": {\n",
    "                        \"scheduler\": scheduler,\n",
    "                        \"monitor\": \"valid/acc\",\n",
    "                        \"interval\": \"epoch\",\n",
    "                        \"frequency\": 1\n",
    "                    }\n",
    "                }\n",
    "            else:\n",
    "                raise ValueError(f\"Scheduler {self.training_config.scheduler} not supported.\")\n",
    "\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n",
    "\n",
    "\n",
    "**Write the methods or classes to be used for training and validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def training_validation(training_config: TrainingConfiguration,\n",
    "                        data_config: DataConfiguration,\n",
    "                        system_config: SystemConfiguration,\n",
    "                        model, data_module):\n",
    "\n",
    "    #random seed for reproducibility\n",
    "    L.seed_everything(training_config.random_seed)\n",
    "\n",
    "    if not model:\n",
    "        raise ValueError(\"Model must be provided for training. Please initialize the model before calling this function.\")\n",
    "    if not data_module:\n",
    "        raise ValueError(\" data module is required to run the model\")\n",
    "    model = model\n",
    "    data_module = data_module\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=system_config.output_dir,\n",
    "        filename=\"{epoch}-{val_loss:.2f}\",\n",
    "        save_top_k=3,\n",
    "        monitor=\"valid/acc\",\n",
    "        mode=\"max\",\n",
    "        auto_insert_metric_name=False,\n",
    "        save_weights_only=True)\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor=\"valid/acc\",\n",
    "        patience=3,\n",
    "        mode=\"max\")\n",
    "\n",
    "    # TensorBoard logger\n",
    "    tensorboard_logger = TensorBoardLogger(\n",
    "        save_dir=system_config.output_dir,\n",
    "        name=\"kenyan_food_logs\",\n",
    "        version=None,  # Auto-incrementing version\n",
    "        default_hp_metric=False\n",
    "    )\n",
    "\n",
    "    # Map precision string to PyTorch Lightning expected value\n",
    "    precision_map = {\n",
    "        \"float32\": 32,\n",
    "        \"float16\": 16,\n",
    "        \"bfloat16\": \"bf16\"\n",
    "    }\n",
    "    trainer_precision = precision_map.get(training_config.precision, 32)\n",
    "\n",
    "    # Map device to accelerator type\n",
    "    accelerator = \"gpu\" if system_config.device == \"cuda\" else \"cpu\"\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=training_config.num_epochs,\n",
    "        accelerator=accelerator,\n",
    "        devices=\"auto\",\n",
    "        precision=trainer_precision,\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "        logger=tensorboard_logger,\n",
    "        default_root_dir=system_config.output_dir,\n",
    "        log_every_n_steps=training_config.log_interval\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "    trainer.validate(model, datamodule=data_module)\n",
    "\n",
    "    return model, data_module, checkpoint_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">5. Model [5 Points]</font>\n",
    "\n",
    "**Define your model in this section.**\n",
    "\n",
    "**You are allowed to use any pre-trained model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">6. Utils [5 Points]</font>\n",
    "\n",
    "**Define those methods or classes, which have  not been covered in the above sections.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_dataset_mean_std(annotations_file, img_dir, img_size=(224, 224), sample_size=None):\n",
    "    \"\"\"\n",
    "    Calculate mean and std for the dataset.\n",
    "\n",
    "    Args:\n",
    "        annotations_file: Path to CSV with image filenames\n",
    "        img_dir: Directory containing images\n",
    "        img_size: Tuple of (height, width) to resize images to\n",
    "        sample_size: If provided, only use this many images for calculation (for speed)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (mean, std) as lists of 3 values each for RGB channels\n",
    "    \"\"\"\n",
    "    # if g_local_run:\n",
    "    #     base, ext = os.path.splitext(annotations_file)\n",
    "    #     annotations_file = f\"{base}_local{ext}\"\n",
    "\n",
    "    img_labels = pd.read_csv(annotations_file)\n",
    "\n",
    "    # Use subset for faster computation if specified\n",
    "    if sample_size and sample_size < len(img_labels):\n",
    "        img_labels = img_labels.sample(n=sample_size, random_state=42)\n",
    "    else: # consider split the labels for train and val in 80:20 ratio and use only train part\n",
    "        total = len(img_labels)\n",
    "        split_index = int(0.8 * total)\n",
    "        img_labels = img_labels.iloc[:split_index].reset_index(drop=True)\n",
    "\n",
    "\n",
    "    print(f\"Calculating mean and std from {len(img_labels)} images...\")\n",
    "\n",
    "    means = []\n",
    "    stds = []\n",
    "\n",
    "    for idx, row in img_labels.iterrows():\n",
    "        img_path = os.path.join(img_dir, str(row.iloc[0])+'.jpg')\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img = img.resize(img_size)\n",
    "            img_array = np.array(img) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "            # Calculate mean and std per channel\n",
    "            img_tensor = torch.tensor(img_array).permute(2, 0, 1)  # C, H, W\n",
    "            means.append(img_tensor.mean(dim=(1, 2)))\n",
    "            stds.append(img_tensor.std(dim=(1, 2)))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Calculate overall mean and std\n",
    "    mean = torch.stack(means).mean(dim=0).tolist()\n",
    "    std = torch.stack(stds).mean(dim=0).tolist()\n",
    "\n",
    "    print(f\"Calculated mean: {mean}\")\n",
    "    print(f\"Calculated std: {std}\")\n",
    "\n",
    "    return mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating mean and std from 30 images...\n",
      "Calculated mean: [0.5725091494931306, 0.47410771600306784, 0.38560582045318126]\n",
      "Calculated std: [0.24724059538343443, 0.25562269849094044, 0.2496358818918856]\n",
      "Calculated mean: [0.5725091494931306, 0.47410771600306784, 0.38560582045318126]\n",
      "Calculated std: [0.24724059538343443, 0.25562269849094044, 0.2496358818918856]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/googlenet-1378be20.pth\" to /root/.cache/torch/hub/checkpoints/googlenet-1378be20.pth\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49.7M/49.7M [00:00<00:00, 185MB/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# mean, std = calculate_dataset_mean_std(\n",
    "#     annotations_file=data_config.annotations_file,\n",
    "#     img_dir=data_config.img_dir,\n",
    "#     img_size=(data_config.input_size, data_config.input_size),\n",
    "#     sample_size=1000  # Use subset for faster calculation, or None for all images\n",
    "# )\n",
    "\n",
    "# Then create DataModule with calculated values:\n",
    "# data_module = KenyanFood13DataModule(data_config, mean=mean, std=std)\n",
    "\n",
    "# Or use default ImageNet stats:\n",
    "# data_module = KenyanFood13DataModule(data_config)\n",
    "\n",
    "if g_measure_mean_std:\n",
    "    mean, std = calculate_dataset_mean_std(\n",
    "        annotations_file=data_config.annotations_file,\n",
    "        img_dir=data_config.img_dir,\n",
    "        img_size=(data_config.input_size, data_config.input_size),\n",
    "        sample_size=None)  # Use subset for faster calculation, or None for all images\n",
    "    print(\"Calculated dataset mean and std:\")\n",
    "    print(\" Mean: {%f}, Std: {%f}\" % (mean, std))\n",
    "    # mean = [0.485, 0.456, 0.406]\n",
    "    # std = [0.229, 0.224, 0.225]\n",
    "    data_module = KenyanFood13DataModule(data_config=data_config, mean=mean, std=std)\n",
    "else:\n",
    "    data_module = KenyanFood13DataModule(data_config=data_config)\n",
    "\n",
    "# Setup data module to get num_classes\n",
    "data_module.setup()\n",
    "\n",
    "model = KenyanFood13Classifier(train_config, data_module.num_classes)\n",
    "\n",
    "# ds = data_module.train_dataset\n",
    "# print(\"dataset class module:\", ds.__class__.__module__)\n",
    "# print(\"dataset class qualname:\", ds.__class__.__qualname__)\n",
    "\n",
    "\n",
    "# # Verification cell inserted by assistant: check dataloaders and one sample batch\n",
    "# # This cell calls data_module.train_dataloader() and data_module.val_dataloader(),\n",
    "# # fetches one batch and prints tensor shapes and label distributions.\n",
    "# try:\n",
    "#     print(\"Verifying dataloaders...\")\n",
    "#     train_loader = data_module.train_dataloader()\n",
    "#     val_loader = data_module.val_dataloader()\n",
    "#     print(f\"Train loader: {train_loader}\")\n",
    "#     print(f\"Val loader: {val_loader}\")\n",
    "\n",
    "#     def inspect_loader(loader, name=\"loader\"):\n",
    "#         it = iter(loader)\n",
    "#         batch = next(it)\n",
    "#         images, labels = batch\n",
    "#         print(f\"{name} - images.shape: {images.shape}, labels.shape: {labels.shape}\")\n",
    "#         try:\n",
    "#             unique, counts = torch.unique(labels, return_counts=True)\n",
    "#             print(f\"{name} - label counts in batch: {dict(zip(unique.tolist(), counts.tolist()))}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Could not compute label counts: {e}\")\n",
    "\n",
    "#     inspect_loader(train_loader, 'train')\n",
    "#     inspect_loader(val_loader, 'val')\n",
    "# except Exception as e:\n",
    "#     print(\"Error while verifying dataloaders:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# t = data_module.train_dataset.transform\n",
    "# try:\n",
    "#     pickle.dumps(t)\n",
    "#     print(\"transform pickles OK\")\n",
    "# except Exception as e:\n",
    "#     print(\"transform pickling failed:\", type(e), e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 42\n",
      "INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:881: Checkpoint directory /kaggle/working/output exists and is not empty.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:881: Checkpoint directory /kaggle/working/output exists and is not empty.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">    </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name            </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>â”ƒ\n",
       "â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0  </span>â”‚ model           â”‚ GoogLeNet           â”‚  5.6 M â”‚ train â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1  </span>â”‚ criterion       â”‚ CrossEntropyLoss    â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2  </span>â”‚ train_mean_loss â”‚ MeanMetric          â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3  </span>â”‚ val_mean_loss   â”‚ MeanMetric          â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4  </span>â”‚ train_accuracy  â”‚ MulticlassAccuracy  â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5  </span>â”‚ val_accuracy    â”‚ MulticlassAccuracy  â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6  </span>â”‚ train_f1        â”‚ MulticlassF1Score   â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7  </span>â”‚ val_f1          â”‚ MulticlassF1Score   â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8  </span>â”‚ train_precision â”‚ MulticlassPrecision â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9  </span>â”‚ val_precision   â”‚ MulticlassPrecision â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 10 </span>â”‚ train_recall    â”‚ MulticlassRecall    â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 11 </span>â”‚ val_recall      â”‚ MulticlassRecall    â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mName           \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mType               \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0mâ”‚ model           â”‚ GoogLeNet           â”‚  5.6 M â”‚ train â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0mâ”‚ criterion       â”‚ CrossEntropyLoss    â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0mâ”‚ train_mean_loss â”‚ MeanMetric          â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0mâ”‚ val_mean_loss   â”‚ MeanMetric          â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0mâ”‚ train_accuracy  â”‚ MulticlassAccuracy  â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0mâ”‚ val_accuracy    â”‚ MulticlassAccuracy  â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0mâ”‚ train_f1        â”‚ MulticlassF1Score   â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0mâ”‚ val_f1          â”‚ MulticlassF1Score   â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0mâ”‚ train_precision â”‚ MulticlassPrecision â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0mâ”‚ val_precision   â”‚ MulticlassPrecision â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0mâ”‚ train_recall    â”‚ MulticlassRecall    â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0mâ”‚ val_recall      â”‚ MulticlassRecall    â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 5.6 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 5.6 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 22                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 235                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 5.6 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 5.6 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 22                                                                         \n",
       "\u001b[1mModules in train mode\u001b[0m: 235                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b8f6146c304e9c8d8d73beac4881b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is \n",
       "incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
       "  self.pid = os.fork()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is \n",
       "incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
       "  self.pid = os.fork()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d300f9de4a44a5bd8ca5563bbbf9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">      Validate metric      </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">           step            </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">           10.0            </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         valid/acc         </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.5774579048156738     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         valid/f1          </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.5689736008644104     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">        valid/loss         </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">     1.14094877243042      </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">      valid/precision      </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.5956078171730042     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">       valid/recall        </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.5774579048156738     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m          step           \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m          10.0           \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        valid/acc        \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.5774579048156738    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        valid/f1         \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.5689736008644104    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m       valid/loss        \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m    1.14094877243042     \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m     valid/precision     \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.5956078171730042    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m      valid/recall       \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.5774579048156738    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, data_module, model_ckpt = training_validation(\n",
    "    training_config=train_config,\n",
    "    data_config=data_config,\n",
    "    system_config=system_config,\n",
    "    model=model,\n",
    "    data_module=data_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">8. TensorBoard Log Link [5 Points]</font>\n",
    "\n",
    "**Share your TensorBoard scalars logs link here You can also share (not mandatory) your GitHub link, if you have pushed this project in GitHub.**\n",
    "\n",
    "\n",
    "Note: In light of the recent shutdown of tensorboard.dev, we have updated the submission requirements for your project. Instead of sharing a tensorboard.dev link, you are now required to upload your generated TensorBoard event files directly onto the lab. As an alternative, you may also include a screenshot of your TensorBoard output within your Jupyter notebook. This adjustment ensures that your data visualization and model training efforts are thoroughly documented and accessible for evaluation.\n",
    "\n",
    "You are also welcome (and encouraged) to utilize alternative logging services like wandB or comet. In such instances, you can easily make your project logs publicly accessible and share the link with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n",
    "\n",
    "**Share your Kaggle profile link  with us here to score , points in  the competition.**\n",
    "\n",
    "**For full points, you need a minimum accuracy of `75%` on the test data. If accuracy is less than `70%`, you gain  no points for this section.**\n",
    "\n",
    "\n",
    "**Submit `submission.csv` (prediction for images in `test.csv`), in the `Submit Predictions` tab in Kaggle, to get evaluated for  this section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10665762,
     "sourceId": 90936,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
